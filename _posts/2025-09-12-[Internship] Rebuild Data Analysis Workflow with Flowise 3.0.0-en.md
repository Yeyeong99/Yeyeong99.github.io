---
title: "[Internship] Rebuilding a Data Analysis Workflow with Flowise 3.0.0" 
date: 2025-09-12 13:15:00 +09:00 
categories: [Internship, AI Agent] 
tags: [internship, ai agent, mcp, data analysis, flowise, rebuilding, workflow, ai] 
description: "Migrating a data analysis workflow from Flowise 2.3.3 to the new Agentflow in Flowise 3.0.0." 
language: en 
postid: 7
---

![The data analysis workflow begins with the user's question. After verifying the file upload, the system determines whether to create a new database or use a pre-existing one. Next, it routes the user's query by categorizing it as either data/domain-related or general/non-domain. If the question pertains to both data and the specific domain, the system generates a formal query. For all other cases, it generates a direct answer. Based on the results of the query, the workflow then visualizes a chart and writes a corresponding report. Finally, if the report lacks sufficient information, the system executes a web search via an MCP to gather additional details.](../assets/img/2025-09-12/data_analysis_workflow.jpg)_Flow Chart Created via Miro_

My most recent task was to rebuild our `Data Analysis Workflow` using the new Flowise v3.0.0. The image above provides a high-level visualization of this process. This post outlines my key areas of focus, the challenges I encountered, and what I learned.

## **Key Areas of Focus**

### 1) Adapting the Original Flow

* **Issue:** It wasn't possible to replicate the original flow exactly in the new version due to architectural changes.
* **Solution:** I consolidated several steps by leveraging `Function Nodes` to create more efficient logic.

### 2) Balancing Adaptability and Specialization

A key challenge was balancing adaptability in the early stages of the flow with specialization in the later stages.

1.  **Flexibility (Database Agnostic):** The database connection is designed to be domain-agnostic. While the current workflow is tailored for a specific domain, the underlying structure allows it to be easily adapted for any other database or data source.
2.  **Specialization (Future Work):** Currently, the workflow uses a generic web search MCP. This can be specialized in the future for specific domains like Financial Analysis, Labor Market Analysis, etc.

### 3) Prompt Engineering

1.  **Strict Instructions:** Prompts must be strict enough to correctly comprehend the user's intent and return a proper answer.
2.  **Output Precision:** The output format must be perfectly precise.
3.  **Addressing Korean Sentence Structure:** Since Korean often follows a Subject-Object-Verb (SOV) structure, the core intent (the verb) can appear at the very end of a sentence. This can cause the LLM to miss the context.
    * For example, in a sentence like, "You should keep in mind that this must be excluded," the key point is "exclusion."
    * To mitigate this, I intentionally rephrased prompts to place the verb earlier, such as, "What you must exclude is..."
4.  **Proper Markdown Formatting:** Using Markdown in prompts helps structure the instructions for the LLM more explicitly.

### 4) Choosing the Right Nodes

* **`Conditional Node` vs. an `LLM` + `Condition Node` Combination:**
    * A simple `Conditional Node` is sufficient if you only need to route the flow based on a scenario (e.g., "Data-related," "Non-domain").
    * However, if you need both the scenario *and* the reasoning behind the classification, combining an `LLM Node` (to generate a JSON output with the scenario and reason) with a `Condition Node` (to route based on the JSON) is a more powerful approach.

### 5) State Management

1.  My goal was to minimize state usage to conserve resources.
2.  However, in some cases, using the flow state is necessary:
* When the output of one node is required by another, non-adjacent node.
* To access specific values from a JSON output generated by a previous node.

## **Feedback I Received**

### 1. Error Handling:
* **Issue:** After several validation loops, raw error messages could be exposed directly to the user.
* **Solution:** The solution was to add a dedicated path in the router to handle this failure case gracefully, providing a user-friendly message instead.

## **Issues I Encountered**

### 1. SQL Query Complexity:
* **Issue:** The LLM tended to generate overly complex SQL queries, prioritizing syntactic correctness over simplicity and efficiency.
* **Solution:** I revised the prompt to enforce that creating the simplest and most direct query possible was the primary goal.
### 2. Chart Creation Errors:
* **Issue:** The `Agent Node`, tasked with creating charts via an MCP, would have the LLM generate the required metadata. However, this metadata often deviated from the format the MCP could parse.
* **Solution:** I implemented a small fix in the MCP code to make it more flexible, allowing it to parse `datasets` directly instead of just single `y-values`.

## **Key Accomplishments**

### 1. Understood the True Power of AI Agents:
- I initially expected the agent to call the chart-generation tool only once per query. However, I observed that if it determined multiple visualizations were necessary, it would autonomously call the tool multiple times to fulfill the request.

### 2. Understood the Difference Between a Simple Package and an MCP Tool:
      
- I've summarized the key differences in the table below.

    | Aspect | **Simple Package / Library** | **MCP (Model-Called Procedure / Tool)** |
    | :--- | :--- | :--- |
    | **Executor** | üë®‚Äçüíª Developer (User) | ü§ñ AI Model (LLM) |
    | **Environment** | User's environment (local machine, server) | A secure, isolated sandbox provided by the AI platform |
    | **Usage Method** | Imported into code and called directly | Called autonomously by the model when it deems necessary |
    | **Purpose** | To integrate necessary functions into an application | To extend the model's capabilities by delegating tasks |
    | **Analogy** | **Tools in a toolbox (drill, hammer):** You use them yourself. | **Skills of a smart assistant (weather, smart home):** The assistant uses them for you. |

